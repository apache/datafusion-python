datafusion
==========

.. py:module:: datafusion

.. autoapi-nested-parse::

   DataFusion python package.

   This is a Python library that binds to Apache Arrow in-memory query engine DataFusion.
   See https://datafusion.apache.org/python for more information.



Submodules
----------

.. toctree::
   :maxdepth: 1

   /autoapi/datafusion/catalog/index
   /autoapi/datafusion/context/index
   /autoapi/datafusion/dataframe/index
   /autoapi/datafusion/dataframe_formatter/index
   /autoapi/datafusion/expr/index
   /autoapi/datafusion/functions/index
   /autoapi/datafusion/html_formatter/index
   /autoapi/datafusion/input/index
   /autoapi/datafusion/io/index
   /autoapi/datafusion/object_store/index
   /autoapi/datafusion/options/index
   /autoapi/datafusion/plan/index
   /autoapi/datafusion/record_batch/index
   /autoapi/datafusion/substrait/index
   /autoapi/datafusion/unparser/index
   /autoapi/datafusion/user_defined/index


Attributes
----------

.. autoapisummary::

   datafusion.DFSchema
   datafusion.col
   datafusion.column
   datafusion.udaf
   datafusion.udf
   datafusion.udtf
   datafusion.udwf


Classes
-------

.. autoapisummary::

   datafusion.Accumulator
   datafusion.AggregateUDF
   datafusion.Catalog
   datafusion.CsvReadOptions
   datafusion.DataFrameWriteOptions
   datafusion.Database
   datafusion.ExecutionPlan
   datafusion.Expr
   datafusion.InsertOp
   datafusion.LogicalPlan
   datafusion.ParquetColumnOptions
   datafusion.ParquetWriterOptions
   datafusion.RecordBatch
   datafusion.RecordBatchStream
   datafusion.RuntimeEnvBuilder
   datafusion.SQLOptions
   datafusion.ScalarUDF
   datafusion.SessionConfig
   datafusion.Table
   datafusion.TableFunction
   datafusion.WindowFrame
   datafusion.WindowUDF


Functions
---------

.. autoapisummary::

   datafusion.configure_formatter
   datafusion.lit
   datafusion.literal
   datafusion.read_avro
   datafusion.read_csv
   datafusion.read_json
   datafusion.read_parquet


Package Contents
----------------

.. py:class:: Accumulator

   Defines how an :py:class:`AggregateUDF` accumulates values.


   .. py:method:: evaluate() -> pyarrow.Scalar
      :abstractmethod:


      Return the resultant value.

      While this function template expects a PyArrow Scalar value return type,
      you can return any value that can be converted into a Scalar. This
      includes basic Python data types such as integers and strings. In
      addition to primitive types, we currently support PyArrow, nanoarrow,
      and arro3 objects in addition to primitive data types. Other objects
      that support the Arrow FFI standard will be given a "best attempt" at
      conversion to scalar objects.



   .. py:method:: merge(states: list[pyarrow.Array]) -> None
      :abstractmethod:


      Merge a set of states.



   .. py:method:: state() -> list[pyarrow.Scalar]
      :abstractmethod:


      Return the current state.

      While this function template expects PyArrow Scalar values return type,
      you can return any value that can be converted into a Scalar. This
      includes basic Python data types such as integers and strings. In
      addition to primitive types, we currently support PyArrow, nanoarrow,
      and arro3 objects in addition to primitive data types. Other objects
      that support the Arrow FFI standard will be given a "best attempt" at
      conversion to scalar objects.



   .. py:method:: update(*values: pyarrow.Array) -> None
      :abstractmethod:


      Evaluate an array of values and update state.



.. py:class:: AggregateUDF(name: str, accumulator: collections.abc.Callable[[], Accumulator], input_types: list[pyarrow.DataType], return_type: pyarrow.DataType, state_type: list[pyarrow.DataType], volatility: Volatility | str)
              AggregateUDF(name: str, accumulator: AggregateUDFExportable, input_types: None = ..., return_type: None = ..., state_type: None = ..., volatility: None = ...)

   Class for performing scalar user-defined functions (UDF).

   Aggregate UDFs operate on a group of rows and return a single value. See
   also :py:class:`ScalarUDF` for operating on a row by row basis.

   Instantiate a user-defined aggregate function (UDAF).

   See :py:func:`udaf` for a convenience function and argument
   descriptions.


   .. py:method:: __call__(*args: datafusion.expr.Expr) -> datafusion.expr.Expr

      Execute the UDAF.

      This function is not typically called by an end user. These calls will
      occur during the evaluation of the dataframe.



   .. py:method:: __repr__() -> str

      Print a string representation of the Aggregate UDF.



   .. py:method:: from_pycapsule(func: AggregateUDFExportable | _typeshed.CapsuleType) -> AggregateUDF
      :staticmethod:


      Create an Aggregate UDF from AggregateUDF PyCapsule object.

      This function will instantiate a Aggregate UDF that uses a DataFusion
      AggregateUDF that is exported via the FFI bindings.



   .. py:method:: udaf(input_types: pyarrow.DataType | list[pyarrow.DataType], return_type: pyarrow.DataType, state_type: list[pyarrow.DataType], volatility: Volatility | str, name: str | None = None) -> collections.abc.Callable[Ellipsis, AggregateUDF]
                  udaf(accum: collections.abc.Callable[[], Accumulator], input_types: pyarrow.DataType | list[pyarrow.DataType], return_type: pyarrow.DataType, state_type: list[pyarrow.DataType], volatility: Volatility | str, name: str | None = None) -> AggregateUDF
                  udaf(accum: AggregateUDFExportable) -> AggregateUDF
                  udaf(accum: _typeshed.CapsuleType) -> AggregateUDF
      :staticmethod:


      Create a new User-Defined Aggregate Function (UDAF).

      This class allows you to define an aggregate function that can be used in
      data aggregation or window function calls.

      Usage:
          - As a function: ``udaf(accum, input_types, return_type, state_type, volatility, name)``.
          - As a decorator: ``@udaf(input_types, return_type, state_type, volatility, name)``.
            When using ``udaf`` as a decorator, do not pass ``accum`` explicitly.

      Function example:

      If your :py:class:`Accumulator` can be instantiated with no arguments, you
      can simply pass it's type as `accum`. If you need to pass additional
      arguments to it's constructor, you can define a lambda or a factory method.
      During runtime the :py:class:`Accumulator` will be constructed for every
      instance in which this UDAF is used. The following examples are all valid::

          import pyarrow as pa
          import pyarrow.compute as pc

          class Summarize(Accumulator):
              def __init__(self, bias: float = 0.0):
                  self._sum = pa.scalar(bias)

              def state(self) -> list[pa.Scalar]:
                  return [self._sum]

              def update(self, values: pa.Array) -> None:
                  self._sum = pa.scalar(self._sum.as_py() + pc.sum(values).as_py())

              def merge(self, states: list[pa.Array]) -> None:
                  self._sum = pa.scalar(self._sum.as_py() + pc.sum(states[0]).as_py())

              def evaluate(self) -> pa.Scalar:
                  return self._sum

          def sum_bias_10() -> Summarize:
              return Summarize(10.0)

          udaf1 = udaf(Summarize, pa.float64(), pa.float64(), [pa.float64()],
              "immutable")
          udaf2 = udaf(sum_bias_10, pa.float64(), pa.float64(), [pa.float64()],
              "immutable")
          udaf3 = udaf(lambda: Summarize(20.0), pa.float64(), pa.float64(),
              [pa.float64()], "immutable")

      Decorator example:::

          @udaf(pa.float64(), pa.float64(), [pa.float64()], "immutable")
          def udf4() -> Summarize:
              return Summarize(10.0)

      :param accum: The accumulator python function. Only needed when calling as a
                    function. Skip this argument when using ``udaf`` as a decorator.
                    If you have a Rust backed AggregateUDF within a PyCapsule, you can
                    pass this parameter and ignore the rest. They will be determined
                    directly from the underlying function. See the online documentation
                    for more information.
      :param input_types: The data types of the arguments to ``accum``.
      :param return_type: The data type of the return value.
      :param state_type: The data types of the intermediate accumulation.
      :param volatility: See :py:class:`Volatility` for allowed values.
      :param name: A descriptive name for the function.

      :returns: A user-defined aggregate function, which can be used in either data
                aggregation or window function calls.



   .. py:attribute:: _udaf


.. py:class:: Catalog(catalog: datafusion._internal.catalog.RawCatalog)

   DataFusion data catalog.

   This constructor is not typically called by the end user.


   .. py:method:: __repr__() -> str

      Print a string representation of the catalog.



   .. py:method:: database(name: str = 'public') -> Schema

      Returns the database with the given ``name`` from this catalog.



   .. py:method:: deregister_schema(name: str, cascade: bool = True) -> Schema | None

      Deregister a schema from this catalog.



   .. py:method:: memory_catalog(ctx: datafusion.SessionContext | None = None) -> Catalog
      :staticmethod:


      Create an in-memory catalog provider.



   .. py:method:: names() -> set[str]

      This is an alias for `schema_names`.



   .. py:method:: register_schema(name: str, schema: Schema | SchemaProvider | SchemaProviderExportable) -> Schema | None

      Register a schema with this catalog.



   .. py:method:: schema(name: str = 'public') -> Schema

      Returns the database with the given ``name`` from this catalog.



   .. py:method:: schema_names() -> set[str]

      Returns the list of schemas in this catalog.



   .. py:attribute:: catalog


.. py:class:: CsvReadOptions(*, has_header: bool = True, delimiter: str = ',', quote: str = '"', terminator: str | None = None, escape: str | None = None, comment: str | None = None, newlines_in_values: bool = False, schema: pyarrow.Schema | None = None, schema_infer_max_records: int = DEFAULT_MAX_INFER_SCHEMA, file_extension: str = '.csv', table_partition_cols: list[tuple[str, pyarrow.DataType]] | None = None, file_compression_type: str = '', file_sort_order: list[list[datafusion.expr.SortExpr]] | None = None, null_regex: str | None = None, truncated_rows: bool = False)

   Options for reading CSV files.

   This class provides a builder pattern for configuring CSV reading options.
   All methods starting with ``with_`` return ``self`` to allow method chaining.

   Initialize CsvReadOptions.

   :param has_header: Does the CSV file have a header row? If schema inference
                      is run on a file with no headers, default column names are created.
   :param delimiter: Column delimiter character. Must be a single ASCII character.
   :param quote: Quote character for fields containing delimiters or newlines.
                 Must be a single ASCII character.
   :param terminator: Optional line terminator character. If ``None``, uses CRLF.
                      Must be a single ASCII character.
   :param escape: Optional escape character for quotes. Must be a single ASCII
                  character.
   :param comment: If specified, lines beginning with this character are ignored.
                   Must be a single ASCII character.
   :param newlines_in_values: Whether newlines in quoted values are supported.
                              Parsing newlines in quoted values may be affected by execution
                              behavior such as parallel file scanning. Setting this to ``True``
                              ensures that newlines in values are parsed successfully, which may
                              reduce performance.
   :param schema: Optional PyArrow schema representing the CSV files. If ``None``,
                  the CSV reader will try to infer it based on data in the file.
   :param schema_infer_max_records: Maximum number of rows to read from CSV files
                                    for schema inference if needed.
   :param file_extension: File extension; only files with this extension are
                          selected for data input.
   :param table_partition_cols: Partition columns as a list of tuples of
                                (column_name, data_type).
   :param file_compression_type: File compression type. Supported values are
                                 ``"gzip"``, ``"bz2"``, ``"xz"``, ``"zstd"``, or empty string for
                                 uncompressed.
   :param file_sort_order: Optional sort order of the files as a list of sort
                           expressions per file.
   :param null_regex: Optional regex pattern to match null values in the CSV.
   :param truncated_rows: Whether to allow truncated rows when parsing. By default
                          this is ``False`` and will error if the CSV rows have different
                          lengths. When set to ``True``, it will allow records with less than
                          the expected number of columns and fill the missing columns with
                          nulls. If the record's schema is not nullable, it will still return
                          an error.


   .. py:method:: to_inner() -> datafusion._internal.options.CsvReadOptions

      Convert this object into the underlying Rust structure.

      This is intended for internal use only.



   .. py:method:: with_comment(comment: str | None) -> CsvReadOptions

      Configure the comment character.



   .. py:method:: with_delimiter(delimiter: str) -> CsvReadOptions

      Configure the column delimiter.



   .. py:method:: with_escape(escape: str | None) -> CsvReadOptions

      Configure the escape character.



   .. py:method:: with_file_compression_type(file_compression_type: str) -> CsvReadOptions

      Configure file compression type.



   .. py:method:: with_file_extension(file_extension: str) -> CsvReadOptions

      Configure the file extension filter.



   .. py:method:: with_file_sort_order(file_sort_order: list[list[datafusion.expr.SortExpr]]) -> CsvReadOptions

      Configure file sort order.



   .. py:method:: with_has_header(has_header: bool) -> CsvReadOptions

      Configure whether the CSV has a header row.



   .. py:method:: with_newlines_in_values(newlines_in_values: bool) -> CsvReadOptions

      Configure whether newlines in values are supported.



   .. py:method:: with_null_regex(null_regex: str | None) -> CsvReadOptions

      Configure null value regex pattern.



   .. py:method:: with_quote(quote: str) -> CsvReadOptions

      Configure the quote character.



   .. py:method:: with_schema(schema: pyarrow.Schema | None) -> CsvReadOptions

      Configure the schema.



   .. py:method:: with_schema_infer_max_records(schema_infer_max_records: int) -> CsvReadOptions

      Configure maximum records for schema inference.



   .. py:method:: with_table_partition_cols(table_partition_cols: list[tuple[str, pyarrow.DataType]]) -> CsvReadOptions

      Configure table partition columns.



   .. py:method:: with_terminator(terminator: str | None) -> CsvReadOptions

      Configure the line terminator character.



   .. py:method:: with_truncated_rows(truncated_rows: bool) -> CsvReadOptions

      Configure whether to allow truncated rows.



   .. py:attribute:: comment
      :value: None



   .. py:attribute:: delimiter
      :value: ','



   .. py:attribute:: escape
      :value: None



   .. py:attribute:: file_compression_type
      :value: ''



   .. py:attribute:: file_extension
      :value: '.csv'



   .. py:attribute:: file_sort_order
      :value: []



   .. py:attribute:: has_header
      :value: True



   .. py:attribute:: newlines_in_values
      :value: False



   .. py:attribute:: null_regex
      :value: None



   .. py:attribute:: quote
      :value: '"'



   .. py:attribute:: schema
      :value: None



   .. py:attribute:: schema_infer_max_records
      :value: 1000



   .. py:attribute:: table_partition_cols
      :value: []



   .. py:attribute:: terminator
      :value: None



   .. py:attribute:: truncated_rows
      :value: False



.. py:class:: DataFrameWriteOptions(insert_operation: InsertOp | None = None, single_file_output: bool = False, partition_by: str | collections.abc.Sequence[str] | None = None, sort_by: datafusion.expr.Expr | datafusion.expr.SortExpr | collections.abc.Sequence[datafusion.expr.Expr] | collections.abc.Sequence[datafusion.expr.SortExpr] | None = None)

   Writer options for DataFrame.

   There is no guarantee the table provider supports all writer options.
   See the individual implementation and documentation for details.

   Instantiate writer options for DataFrame.


   .. py:attribute:: _raw_write_options


.. py:class:: Database(schema: datafusion._internal.catalog.RawSchema)

   Bases: :py:obj:`Schema`


   See `Schema`.

   This constructor is not typically called by the end user.


.. py:class:: ExecutionPlan(plan: datafusion._internal.ExecutionPlan)

   Represent nodes in the DataFusion Physical Plan.

   This constructor should not be called by the end user.


   .. py:method:: __repr__() -> str

      Print a string representation of the physical plan.



   .. py:method:: children() -> list[ExecutionPlan]

      Get a list of children `ExecutionPlan` that act as inputs to this plan.

      The returned list will be empty for leaf nodes such as scans, will contain a
      single value for unary nodes, or two values for binary nodes (such as joins).



   .. py:method:: display() -> str

      Print the physical plan.



   .. py:method:: display_indent() -> str

      Print an indented form of the physical plan.



   .. py:method:: from_proto(ctx: datafusion.context.SessionContext, data: bytes) -> ExecutionPlan
      :staticmethod:


      Create an ExecutionPlan from protobuf bytes.

      Tables created in memory from record batches are currently not supported.



   .. py:method:: to_proto() -> bytes

      Convert an ExecutionPlan into protobuf bytes.

      Tables created in memory from record batches are currently not supported.



   .. py:attribute:: _raw_plan


   .. py:property:: partition_count
      :type: int


      Returns the number of partitions in the physical plan.


.. py:class:: Expr(expr: datafusion._internal.expr.RawExpr)

   Expression object.

   Expressions are one of the core concepts in DataFusion. See
   :ref:`Expressions` in the online documentation for more information.

   This constructor should not be called by the end user.


   .. py:method:: __add__(rhs: Any) -> Expr

      Addition operator.

      Accepts either an expression or any valid PyArrow scalar literal value.



   .. py:method:: __and__(rhs: Expr) -> Expr

      Logical AND.



   .. py:method:: __eq__(rhs: object) -> Expr

      Equal to.

      Accepts either an expression or any valid PyArrow scalar literal value.



   .. py:method:: __ge__(rhs: Any) -> Expr

      Greater than or equal to.

      Accepts either an expression or any valid PyArrow scalar literal value.



   .. py:method:: __getitem__(key: str | int) -> Expr

      Retrieve sub-object.

      If ``key`` is a string, returns the subfield of the struct.
      If ``key`` is an integer, retrieves the element in the array. Note that the
      element index begins at ``0``, unlike
      :py:func:`~datafusion.functions.array_element` which begins at ``1``.
      If ``key`` is a slice, returns an array that contains a slice of the
      original array. Similar to integer indexing, this follows Python convention
      where the index begins at ``0`` unlike
      :py:func:`~datafusion.functions.array_slice` which begins at ``1``.



   .. py:method:: __gt__(rhs: Any) -> Expr

      Greater than.

      Accepts either an expression or any valid PyArrow scalar literal value.



   .. py:method:: __invert__() -> Expr

      Binary not (~).



   .. py:method:: __le__(rhs: Any) -> Expr

      Less than or equal to.

      Accepts either an expression or any valid PyArrow scalar literal value.



   .. py:method:: __lt__(rhs: Any) -> Expr

      Less than.

      Accepts either an expression or any valid PyArrow scalar literal value.



   .. py:method:: __mod__(rhs: Any) -> Expr

      Modulo operator (%).

      Accepts either an expression or any valid PyArrow scalar literal value.



   .. py:method:: __mul__(rhs: Any) -> Expr

      Multiplication operator.

      Accepts either an expression or any valid PyArrow scalar literal value.



   .. py:method:: __ne__(rhs: object) -> Expr

      Not equal to.

      Accepts either an expression or any valid PyArrow scalar literal value.



   .. py:method:: __or__(rhs: Expr) -> Expr

      Logical OR.



   .. py:method:: __repr__() -> str

      Generate a string representation of this expression.



   .. py:method:: __richcmp__(other: Expr, op: int) -> Expr

      Comparison operator.



   .. py:method:: __sub__(rhs: Any) -> Expr

      Subtraction operator.

      Accepts either an expression or any valid PyArrow scalar literal value.



   .. py:method:: __truediv__(rhs: Any) -> Expr

      Division operator.

      Accepts either an expression or any valid PyArrow scalar literal value.



   .. py:method:: abs() -> Expr

      Return the absolute value of a given number.

      Returns:
      --------
      Expr
          A new expression representing the absolute value of the input expression.



   .. py:method:: acos() -> Expr

      Returns the arc cosine or inverse cosine of a number.

      Returns:
      --------
      Expr
          A new expression representing the arc cosine of the input expression.



   .. py:method:: acosh() -> Expr

      Returns inverse hyperbolic cosine.



   .. py:method:: alias(name: str, metadata: dict[str, str] | None = None) -> Expr

      Assign a name to the expression.

      :param name: The name to assign to the expression.
      :param metadata: Optional metadata to attach to the expression.

      :returns: A new expression with the assigned name.



   .. py:method:: array_dims() -> Expr

      Returns an array of the array's dimensions.



   .. py:method:: array_distinct() -> Expr

      Returns distinct values from the array after removing duplicates.



   .. py:method:: array_empty() -> Expr

      Returns a boolean indicating whether the array is empty.



   .. py:method:: array_length() -> Expr

      Returns the length of the array.



   .. py:method:: array_ndims() -> Expr

      Returns the number of dimensions of the array.



   .. py:method:: array_pop_back() -> Expr

      Returns the array without the last element.



   .. py:method:: array_pop_front() -> Expr

      Returns the array without the first element.



   .. py:method:: arrow_typeof() -> Expr

      Returns the Arrow type of the expression.



   .. py:method:: ascii() -> Expr

      Returns the numeric code of the first character of the argument.



   .. py:method:: asin() -> Expr

      Returns the arc sine or inverse sine of a number.



   .. py:method:: asinh() -> Expr

      Returns inverse hyperbolic sine.



   .. py:method:: atan() -> Expr

      Returns inverse tangent of a number.



   .. py:method:: atanh() -> Expr

      Returns inverse hyperbolic tangent.



   .. py:method:: between(low: Any, high: Any, negated: bool = False) -> Expr

      Returns ``True`` if this expression is between a given range.

      :param low: lower bound of the range (inclusive).
      :param high: higher bound of the range (inclusive).
      :param negated: negates whether the expression is between a given range



   .. py:method:: bit_length() -> Expr

      Returns the number of bits in the string argument.



   .. py:method:: btrim() -> Expr

      Removes all characters, spaces by default, from both sides of a string.



   .. py:method:: canonical_name() -> str

      Returns a complete string representation of this expression.



   .. py:method:: cardinality() -> Expr

      Returns the total number of elements in the array.



   .. py:method:: cast(to: pyarrow.DataType[Any] | type) -> Expr

      Cast to a new data type.



   .. py:method:: cbrt() -> Expr

      Returns the cube root of a number.



   .. py:method:: ceil() -> Expr

      Returns the nearest integer greater than or equal to argument.



   .. py:method:: char_length() -> Expr

      The number of characters in the ``string``.



   .. py:method:: character_length() -> Expr

      Returns the number of characters in the argument.



   .. py:method:: chr() -> Expr

      Converts the Unicode code point to a UTF8 character.



   .. py:method:: column(value: str) -> Expr
      :staticmethod:


      Creates a new expression representing a column.



   .. py:method:: column_name(plan: datafusion.plan.LogicalPlan) -> str

      Compute the output column name based on the provided logical plan.



   .. py:method:: cos() -> Expr

      Returns the cosine of the argument.



   .. py:method:: cosh() -> Expr

      Returns the hyperbolic cosine of the argument.



   .. py:method:: cot() -> Expr

      Returns the cotangent of the argument.



   .. py:method:: degrees() -> Expr

      Converts the argument from radians to degrees.



   .. py:method:: display_name() -> str

      Returns the name of this expression as it should appear in a schema.

      This name will not include any CAST expressions.



   .. py:method:: distinct() -> ExprFuncBuilder

      Only evaluate distinct values for an aggregate function.

      This function will create an :py:class:`ExprFuncBuilder` that can be used to
      set parameters for either window or aggregate functions. If used on any other
      type of expression, an error will be generated when ``build()`` is called.



   .. py:method:: empty() -> Expr

      This is an alias for :py:func:`array_empty`.



   .. py:method:: exp() -> Expr

      Returns the exponential of the argument.



   .. py:method:: factorial() -> Expr

      Returns the factorial of the argument.



   .. py:method:: fill_nan(value: Any | Expr | None = None) -> Expr

      Fill NaN values with a provided value.



   .. py:method:: fill_null(value: Any | Expr | None = None) -> Expr

      Fill NULL values with a provided value.



   .. py:method:: filter(filter: Expr) -> ExprFuncBuilder

      Filter an aggregate function.

      This function will create an :py:class:`ExprFuncBuilder` that can be used to
      set parameters for either window or aggregate functions. If used on any other
      type of expression, an error will be generated when ``build()`` is called.



   .. py:method:: flatten() -> Expr

      Flattens an array of arrays into a single array.



   .. py:method:: floor() -> Expr

      Returns the nearest integer less than or equal to the argument.



   .. py:method:: from_unixtime() -> Expr

      Converts an integer to RFC3339 timestamp format string.



   .. py:method:: initcap() -> Expr

      Set the initial letter of each word to capital.

      Converts the first letter of each word in ``string`` to uppercase and the
      remaining characters to lowercase.



   .. py:method:: is_not_null() -> Expr

      Returns ``True`` if this expression is not null.



   .. py:method:: is_null() -> Expr

      Returns ``True`` if this expression is null.



   .. py:method:: isnan() -> Expr

      Returns true if a given number is +NaN or -NaN otherwise returns false.



   .. py:method:: iszero() -> Expr

      Returns true if a given number is +0.0 or -0.0 otherwise returns false.



   .. py:method:: length() -> Expr

      The number of characters in the ``string``.



   .. py:method:: list_dims() -> Expr

      Returns an array of the array's dimensions.

      This is an alias for :py:func:`array_dims`.



   .. py:method:: list_distinct() -> Expr

      Returns distinct values from the array after removing duplicates.

      This is an alias for :py:func:`array_distinct`.



   .. py:method:: list_length() -> Expr

      Returns the length of the array.

      This is an alias for :py:func:`array_length`.



   .. py:method:: list_ndims() -> Expr

      Returns the number of dimensions of the array.

      This is an alias for :py:func:`array_ndims`.



   .. py:method:: literal(value: Any) -> Expr
      :staticmethod:


      Creates a new expression representing a scalar value.

      ``value`` must be a valid PyArrow scalar value or easily castable to one.



   .. py:method:: literal_with_metadata(value: Any, metadata: dict[str, str]) -> Expr
      :staticmethod:


      Creates a new expression representing a scalar value with metadata.

      :param value: A valid PyArrow scalar value or easily castable to one.
      :param metadata: Metadata to attach to the expression.



   .. py:method:: ln() -> Expr

      Returns the natural logarithm (base e) of the argument.



   .. py:method:: log10() -> Expr

      Base 10 logarithm of the argument.



   .. py:method:: log2() -> Expr

      Base 2 logarithm of the argument.



   .. py:method:: lower() -> Expr

      Converts a string to lowercase.



   .. py:method:: ltrim() -> Expr

      Removes all characters, spaces by default, from the beginning of a string.



   .. py:method:: md5() -> Expr

      Computes an MD5 128-bit checksum for a string expression.



   .. py:method:: null_treatment(null_treatment: datafusion.common.NullTreatment) -> ExprFuncBuilder

      Set the treatment for ``null`` values for a window or aggregate function.

      This function will create an :py:class:`ExprFuncBuilder` that can be used to
      set parameters for either window or aggregate functions. If used on any other
      type of expression, an error will be generated when ``build()`` is called.



   .. py:method:: octet_length() -> Expr

      Returns the number of bytes of a string.



   .. py:method:: order_by(*exprs: Expr | SortExpr) -> ExprFuncBuilder

      Set the ordering for a window or aggregate function.

      This function will create an :py:class:`ExprFuncBuilder` that can be used to
      set parameters for either window or aggregate functions. If used on any other
      type of expression, an error will be generated when ``build()`` is called.



   .. py:method:: over(window: Window) -> Expr

      Turn an aggregate function into a window function.

      This function turns any aggregate function into a window function. With the
      exception of ``partition_by``, how each of the parameters is used is determined
      by the underlying aggregate function.

      :param window: Window definition



   .. py:method:: partition_by(*partition_by: Expr) -> ExprFuncBuilder

      Set the partitioning for a window function.

      This function will create an :py:class:`ExprFuncBuilder` that can be used to
      set parameters for either window or aggregate functions. If used on any other
      type of expression, an error will be generated when ``build()`` is called.



   .. py:method:: python_value() -> Any

      Extracts the Expr value into `Any`.

      This is only valid for literal expressions.

      :returns: Python object representing literal value of the expression.



   .. py:method:: radians() -> Expr

      Converts the argument from degrees to radians.



   .. py:method:: reverse() -> Expr

      Reverse the string argument.



   .. py:method:: rex_call_operands() -> list[Expr]

      Return the operands of the expression based on it's variant type.

      Row expressions, Rex(s), operate on the concept of operands. Different
      variants of Expressions, Expr(s), store those operands in different
      datastructures. This function examines the Expr variant and returns
      the operands to the calling logic.



   .. py:method:: rex_call_operator() -> str

      Extracts the operator associated with a row expression type call.



   .. py:method:: rex_type() -> datafusion.common.RexType

      Return the Rex Type of this expression.

      A Rex (Row Expression) specifies a single row of data.That specification
      could include user defined functions or types. RexType identifies the
      row as one of the possible valid ``RexType``.



   .. py:method:: rtrim() -> Expr

      Removes all characters, spaces by default, from the end of a string.



   .. py:method:: schema_name() -> str

      Returns the name of this expression as it should appear in a schema.

      This name will not include any CAST expressions.



   .. py:method:: sha224() -> Expr

      Computes the SHA-224 hash of a binary string.



   .. py:method:: sha256() -> Expr

      Computes the SHA-256 hash of a binary string.



   .. py:method:: sha384() -> Expr

      Computes the SHA-384 hash of a binary string.



   .. py:method:: sha512() -> Expr

      Computes the SHA-512 hash of a binary string.



   .. py:method:: signum() -> Expr

      Returns the sign of the argument (-1, 0, +1).



   .. py:method:: sin() -> Expr

      Returns the sine of the argument.



   .. py:method:: sinh() -> Expr

      Returns the hyperbolic sine of the argument.



   .. py:method:: sort(ascending: bool = True, nulls_first: bool = True) -> SortExpr

      Creates a sort :py:class:`Expr` from an existing :py:class:`Expr`.

      :param ascending: If true, sort in ascending order.
      :param nulls_first: Return null values first.



   .. py:method:: sqrt() -> Expr

      Returns the square root of the argument.



   .. py:method:: string_literal(value: str) -> Expr
      :staticmethod:


      Creates a new expression representing a UTF8 literal value.

      It is different from `literal` because it is pa.string() instead of
      pa.string_view()

      This is needed for cases where DataFusion is expecting a UTF8 instead of
      UTF8View literal, like in:
      https://github.com/apache/datafusion/blob/86740bfd3d9831d6b7c1d0e1bf4a21d91598a0ac/datafusion/functions/src/core/arrow_cast.rs#L179



   .. py:method:: tan() -> Expr

      Returns the tangent of the argument.



   .. py:method:: tanh() -> Expr

      Returns the hyperbolic tangent of the argument.



   .. py:method:: to_hex() -> Expr

      Converts an integer to a hexadecimal string.



   .. py:method:: to_variant() -> Any

      Convert this expression into a python object if possible.



   .. py:method:: trim() -> Expr

      Removes all characters, spaces by default, from both sides of a string.



   .. py:method:: types() -> datafusion.common.DataTypeMap

      Return the ``DataTypeMap``.

      :returns: DataTypeMap which represents the PythonType, Arrow DataType, and
                SqlType Enum which this expression represents.



   .. py:method:: upper() -> Expr

      Converts a string to uppercase.



   .. py:method:: variant_name() -> str

      Returns the name of the Expr variant.

      Ex: ``IsNotNull``, ``Literal``, ``BinaryExpr``, etc



   .. py:method:: window_frame(window_frame: WindowFrame) -> ExprFuncBuilder

      Set the frame fora  window function.

      This function will create an :py:class:`ExprFuncBuilder` that can be used to
      set parameters for either window or aggregate functions. If used on any other
      type of expression, an error will be generated when ``build()`` is called.



   .. py:attribute:: __radd__


   .. py:attribute:: __rand__


   .. py:attribute:: __rmod__


   .. py:attribute:: __rmul__


   .. py:attribute:: __ror__


   .. py:attribute:: __rsub__


   .. py:attribute:: __rtruediv__


   .. py:attribute:: _to_pyarrow_types
      :type:  ClassVar[dict[type, pyarrow.DataType]]


   .. py:attribute:: expr


.. py:class:: InsertOp

   Bases: :py:obj:`enum.Enum`


   Insert operation mode.

   These modes are used by the table writing feature to define how record
   batches should be written to a table.


   .. py:attribute:: APPEND

      Appends new rows to the existing table without modifying any existing rows.


   .. py:attribute:: OVERWRITE

      Overwrites all existing rows in the table with the new rows.


   .. py:attribute:: REPLACE

      Replace existing rows that collide with the inserted rows.

      Replacement is typically based on a unique key or primary key.


.. py:class:: LogicalPlan(plan: datafusion._internal.LogicalPlan)

   Logical Plan.

   A `LogicalPlan` is a node in a tree of relational operators (such as
   Projection or Filter).

   Represents transforming an input relation (table) to an output relation
   (table) with a potentially different schema. Plans form a dataflow tree
   where data flows from leaves up to the root to produce the query result.

   A `LogicalPlan` can be created by the SQL query planner, the DataFrame API,
   or programmatically (for example custom query languages).

   This constructor should not be called by the end user.


   .. py:method:: __eq__(other: LogicalPlan) -> bool

      Test equality.



   .. py:method:: __repr__() -> str

      Generate a printable representation of the plan.



   .. py:method:: display() -> str

      Print the logical plan.



   .. py:method:: display_graphviz() -> str

      Print the graph visualization of the logical plan.

      Returns a `format`able structure that produces lines meant for graphical display
      using the `DOT` language. This format can be visualized using software from
      [`graphviz`](https://graphviz.org/)



   .. py:method:: display_indent() -> str

      Print an indented form of the logical plan.



   .. py:method:: display_indent_schema() -> str

      Print an indented form of the schema for the logical plan.



   .. py:method:: from_proto(ctx: datafusion.context.SessionContext, data: bytes) -> LogicalPlan
      :staticmethod:


      Create a LogicalPlan from protobuf bytes.

      Tables created in memory from record batches are currently not supported.



   .. py:method:: inputs() -> list[LogicalPlan]

      Returns the list of inputs to the logical plan.



   .. py:method:: to_proto() -> bytes

      Convert a LogicalPlan to protobuf bytes.

      Tables created in memory from record batches are currently not supported.



   .. py:method:: to_variant() -> Any

      Convert the logical plan into its specific variant.



   .. py:attribute:: _raw_plan


.. py:class:: ParquetColumnOptions(encoding: str | None = None, dictionary_enabled: bool | None = None, compression: str | None = None, statistics_enabled: str | None = None, bloom_filter_enabled: bool | None = None, bloom_filter_fpp: float | None = None, bloom_filter_ndv: int | None = None)

   Parquet options for individual columns.

   Contains the available options that can be applied for an individual Parquet column,
   replacing the global options in ``ParquetWriterOptions``.

   Initialize the ParquetColumnOptions.

   :param encoding: Sets encoding for the column path. Valid values are: ``plain``,
                    ``plain_dictionary``, ``rle``, ``bit_packed``, ``delta_binary_packed``,
                    ``delta_length_byte_array``, ``delta_byte_array``, ``rle_dictionary``,
                    and ``byte_stream_split``. These values are not case-sensitive. If
                    ``None``, uses the default parquet options
   :param dictionary_enabled: Sets if dictionary encoding is enabled for the column
                              path. If `None`, uses the default parquet options
   :param compression: Sets default parquet compression codec for the column path.
                       Valid values are ``uncompressed``, ``snappy``, ``gzip(level)``, ``lzo``,
                       ``brotli(level)``, ``lz4``, ``zstd(level)``, and ``lz4_raw``. These
                       values are not case-sensitive. If ``None``, uses the default parquet
                       options.
   :param statistics_enabled: Sets if statistics are enabled for the column Valid
                              values are: ``none``, ``chunk``, and ``page`` These values are not case
                              sensitive. If ``None``, uses the default parquet options.
   :param bloom_filter_enabled: Sets if bloom filter is enabled for the column path.
                                If ``None``, uses the default parquet options.
   :param bloom_filter_fpp: Sets bloom filter false positive probability for the
                            column path. If ``None``, uses the default parquet options.
   :param bloom_filter_ndv: Sets bloom filter number of distinct values. If ``None``,
                            uses the default parquet options.


   .. py:attribute:: bloom_filter_enabled
      :value: None



   .. py:attribute:: bloom_filter_fpp
      :value: None



   .. py:attribute:: bloom_filter_ndv
      :value: None



   .. py:attribute:: compression
      :value: None



   .. py:attribute:: dictionary_enabled
      :value: None



   .. py:attribute:: encoding
      :value: None



   .. py:attribute:: statistics_enabled
      :value: None



.. py:class:: ParquetWriterOptions(data_pagesize_limit: int = 1024 * 1024, write_batch_size: int = 1024, writer_version: str = '1.0', skip_arrow_metadata: bool = False, compression: str | None = 'zstd(3)', compression_level: int | None = None, dictionary_enabled: bool | None = True, dictionary_page_size_limit: int = 1024 * 1024, statistics_enabled: str | None = 'page', max_row_group_size: int = 1024 * 1024, created_by: str = 'datafusion-python', column_index_truncate_length: int | None = 64, statistics_truncate_length: int | None = None, data_page_row_count_limit: int = 20000, encoding: str | None = None, bloom_filter_on_write: bool = False, bloom_filter_fpp: float | None = None, bloom_filter_ndv: int | None = None, allow_single_file_parallelism: bool = True, maximum_parallel_row_group_writers: int = 1, maximum_buffered_record_batches_per_stream: int = 2, column_specific_options: dict[str, ParquetColumnOptions] | None = None)

   Advanced parquet writer options.

   Allows settings the writer options that apply to the entire file. Some options can
   also be set on a column by column basis, with the field ``column_specific_options``
   (see ``ParquetColumnOptions``).

   Initialize the ParquetWriterOptions.

   :param data_pagesize_limit: Sets best effort maximum size of data page in bytes.
   :param write_batch_size: Sets write_batch_size in bytes.
   :param writer_version: Sets parquet writer version. Valid values are ``1.0`` and
                          ``2.0``.
   :param skip_arrow_metadata: Skip encoding the embedded arrow metadata in the
                               KV_meta.
   :param compression: Compression type to use. Default is ``zstd(3)``.
                       Available compression types are

                       - ``uncompressed``: No compression.
                       - ``snappy``: Snappy compression.
                       - ``gzip(n)``: Gzip compression with level n.
                       - ``brotli(n)``: Brotli compression with level n.
                       - ``lz4``: LZ4 compression.
                       - ``lz4_raw``: LZ4_RAW compression.
                       - ``zstd(n)``: Zstandard compression with level n.
   :param compression_level: Compression level to set.
   :param dictionary_enabled: Sets if dictionary encoding is enabled. If ``None``,
                              uses the default parquet writer setting.
   :param dictionary_page_size_limit: Sets best effort maximum dictionary page size,
                                      in bytes.
   :param statistics_enabled: Sets if statistics are enabled for any column Valid
                              values are ``none``, ``chunk``, and ``page``. If ``None``, uses the
                              default parquet writer setting.
   :param max_row_group_size: Target maximum number of rows in each row group
                              (defaults to 1M rows). Writing larger row groups requires more memory
                              to write, but can get better compression and be faster to read.
   :param created_by: Sets "created by" property.
   :param column_index_truncate_length: Sets column index truncate length.
   :param statistics_truncate_length: Sets statistics truncate length. If ``None``,
                                      uses the default parquet writer setting.
   :param data_page_row_count_limit: Sets best effort maximum number of rows in a data
                                     page.
   :param encoding: Sets default encoding for any column. Valid values are ``plain``,
                    ``plain_dictionary``, ``rle``, ``bit_packed``, ``delta_binary_packed``,
                    ``delta_length_byte_array``, ``delta_byte_array``, ``rle_dictionary``,
                    and ``byte_stream_split``. If ``None``, uses the default parquet writer
                    setting.
   :param bloom_filter_on_write: Write bloom filters for all columns when creating
                                 parquet files.
   :param bloom_filter_fpp: Sets bloom filter false positive probability. If ``None``,
                            uses the default parquet writer setting
   :param bloom_filter_ndv: Sets bloom filter number of distinct values. If ``None``,
                            uses the default parquet writer setting.
   :param allow_single_file_parallelism: Controls whether DataFusion will attempt to
                                         speed up writing parquet files by serializing them in parallel. Each
                                         column in each row group in each output file are serialized in parallel
                                         leveraging a maximum possible core count of
                                         ``n_files * n_row_groups * n_columns``.
   :param maximum_parallel_row_group_writers: By default parallel parquet writer is
                                              tuned for minimum memory usage in a streaming execution plan. You may
                                              see a performance benefit when writing large parquet files by increasing
                                              ``maximum_parallel_row_group_writers`` and
                                              ``maximum_buffered_record_batches_per_stream`` if your system has idle
                                              cores and can tolerate additional memory usage. Boosting these values is
                                              likely worthwhile when writing out already in-memory data, such as from
                                              a cached data frame.
   :param maximum_buffered_record_batches_per_stream: See
                                                      ``maximum_parallel_row_group_writers``.
   :param column_specific_options: Overrides options for specific columns. If a column
                                   is not a part of this dictionary, it will use the parameters provided
                                   here.


   .. py:attribute:: allow_single_file_parallelism
      :value: True



   .. py:attribute:: bloom_filter_fpp
      :value: None



   .. py:attribute:: bloom_filter_ndv
      :value: None



   .. py:attribute:: bloom_filter_on_write
      :value: False



   .. py:attribute:: column_index_truncate_length
      :value: 64



   .. py:attribute:: column_specific_options
      :value: None



   .. py:attribute:: created_by
      :value: 'datafusion-python'



   .. py:attribute:: data_page_row_count_limit
      :value: 20000



   .. py:attribute:: data_pagesize_limit
      :value: 1048576



   .. py:attribute:: dictionary_enabled
      :value: True



   .. py:attribute:: dictionary_page_size_limit
      :value: 1048576



   .. py:attribute:: encoding
      :value: None



   .. py:attribute:: max_row_group_size
      :value: 1048576



   .. py:attribute:: maximum_buffered_record_batches_per_stream
      :value: 2



   .. py:attribute:: maximum_parallel_row_group_writers
      :value: 1



   .. py:attribute:: skip_arrow_metadata
      :value: False



   .. py:attribute:: statistics_enabled
      :value: 'page'



   .. py:attribute:: statistics_truncate_length
      :value: None



   .. py:attribute:: write_batch_size
      :value: 1024



   .. py:attribute:: writer_version
      :value: '1.0'



.. py:class:: RecordBatch(record_batch: datafusion._internal.RecordBatch)

   This class is essentially a wrapper for :py:class:`pa.RecordBatch`.

   This constructor is generally not called by the end user.

   See the :py:class:`RecordBatchStream` iterator for generating this class.


   .. py:method:: __arrow_c_array__(requested_schema: object | None = None) -> tuple[object, object]

      Export the record batch via the Arrow C Data Interface.

      This allows zero-copy interchange with libraries that support the
      `Arrow PyCapsule interface <https://arrow.apache.org/docs/format/
      CDataInterface/PyCapsuleInterface.html>`_.

      :param requested_schema: Attempt to provide the record batch using this
                               schema. Only straightforward projections such as column
                               selection or reordering are applied.

      :returns: Two Arrow PyCapsule objects representing the ``ArrowArray`` and
                ``ArrowSchema``.



   .. py:method:: to_pyarrow() -> pyarrow.RecordBatch

      Convert to :py:class:`pa.RecordBatch`.



   .. py:attribute:: record_batch


.. py:class:: RecordBatchStream(record_batch_stream: datafusion._internal.RecordBatchStream)

   This class represents a stream of record batches.

   These are typically the result of a
   :py:func:`~datafusion.dataframe.DataFrame.execute_stream` operation.

   This constructor is typically not called by the end user.


   .. py:method:: __aiter__() -> typing_extensions.Self

      Return an asynchronous iterator over record batches.



   .. py:method:: __anext__() -> RecordBatch
      :async:


      Return the next :py:class:`RecordBatch` in the stream asynchronously.



   .. py:method:: __iter__() -> typing_extensions.Self

      Return an iterator over record batches.



   .. py:method:: __next__() -> RecordBatch

      Return the next :py:class:`RecordBatch` in the stream.



   .. py:method:: next() -> RecordBatch

      See :py:func:`__next__` for the iterator function.



   .. py:attribute:: rbs


.. py:class:: RuntimeEnvBuilder

   Runtime configuration options.

   Create a new :py:class:`RuntimeEnvBuilder` with default values.


   .. py:method:: with_disk_manager_disabled() -> RuntimeEnvBuilder

      Disable the disk manager, attempts to create temporary files will error.

      :returns: A new :py:class:`RuntimeEnvBuilder` object with the updated setting.



   .. py:method:: with_disk_manager_os() -> RuntimeEnvBuilder

      Use the operating system's temporary directory for disk manager.

      :returns: A new :py:class:`RuntimeEnvBuilder` object with the updated setting.



   .. py:method:: with_disk_manager_specified(*paths: str | pathlib.Path) -> RuntimeEnvBuilder

      Use the specified paths for the disk manager's temporary files.

      :param paths: Paths to use for the disk manager's temporary files.

      :returns: A new :py:class:`RuntimeEnvBuilder` object with the updated setting.



   .. py:method:: with_fair_spill_pool(size: int) -> RuntimeEnvBuilder

      Use a fair spill pool with the specified size.

      This pool works best when you know beforehand the query has multiple spillable
      operators that will likely all need to spill. Sometimes it will cause spills
      even when there was sufficient memory (reserved for other operators) to avoid
      doing so::

          zz
                                 z                      z               
                                 z                      z               
                 Spillable       z       Unspillable    z     Free      
                  Memory         z        Memory        z    Memory     
                                 z                      z               
                                 z                      z               
          zz

      :param size: Size of the memory pool in bytes.

      :returns: A new :py:class:`RuntimeEnvBuilder` object with the updated setting.

      Examples usage::

          config = RuntimeEnvBuilder().with_fair_spill_pool(1024)



   .. py:method:: with_greedy_memory_pool(size: int) -> RuntimeEnvBuilder

      Use a greedy memory pool with the specified size.

      This pool works well for queries that do not need to spill or have a single
      spillable operator. See :py:func:`with_fair_spill_pool` if there are
      multiple spillable operators that all will spill.

      :param size: Size of the memory pool in bytes.

      :returns: A new :py:class:`RuntimeEnvBuilder` object with the updated setting.

      Example usage::

          config = RuntimeEnvBuilder().with_greedy_memory_pool(1024)



   .. py:method:: with_temp_file_path(path: str | pathlib.Path) -> RuntimeEnvBuilder

      Use the specified path to create any needed temporary files.

      :param path: Path to use for temporary files.

      :returns: A new :py:class:`RuntimeEnvBuilder` object with the updated setting.

      Example usage::

          config = RuntimeEnvBuilder().with_temp_file_path("/tmp")



   .. py:method:: with_unbounded_memory_pool() -> RuntimeEnvBuilder

      Use an unbounded memory pool.

      :returns: A new :py:class:`RuntimeEnvBuilder` object with the updated setting.



   .. py:attribute:: config_internal


.. py:class:: SQLOptions

   Options to be used when performing SQL queries.

   Create a new :py:class:`SQLOptions` with default values.

   The default values are:
   - DDL commands are allowed
   - DML commands are allowed
   - Statements are allowed


   .. py:method:: with_allow_ddl(allow: bool = True) -> SQLOptions

      Should DDL (Data Definition Language) commands be run?

      Examples of DDL commands include ``CREATE TABLE`` and ``DROP TABLE``.

      :param allow: Allow DDL commands to be run.

      :returns: A new :py:class:`SQLOptions` object with the updated setting.

      Example usage::

          options = SQLOptions().with_allow_ddl(True)



   .. py:method:: with_allow_dml(allow: bool = True) -> SQLOptions

      Should DML (Data Manipulation Language) commands be run?

      Examples of DML commands include ``INSERT INTO`` and ``DELETE``.

      :param allow: Allow DML commands to be run.

      :returns: A new :py:class:`SQLOptions` object with the updated setting.

      Example usage::

          options = SQLOptions().with_allow_dml(True)



   .. py:method:: with_allow_statements(allow: bool = True) -> SQLOptions

      Should statements such as ``SET VARIABLE`` and ``BEGIN TRANSACTION`` be run?

      :param allow: Allow statements to be run.

      :returns: py:class:SQLOptions` object with the updated setting.
      :rtype: A new

      Example usage::

          options = SQLOptions().with_allow_statements(True)



   .. py:attribute:: options_internal


.. py:class:: ScalarUDF(name: str, func: collections.abc.Callable[Ellipsis, _R], input_fields: list[pyarrow.Field], return_field: _R, volatility: Volatility | str)

   Class for performing scalar user-defined functions (UDF).

   Scalar UDFs operate on a row by row basis. See also :py:class:`AggregateUDF` for
   operating on a group of rows.

   Instantiate a scalar user-defined function (UDF).

   See helper method :py:func:`udf` for argument details.


   .. py:method:: __call__(*args: datafusion.expr.Expr) -> datafusion.expr.Expr

      Execute the UDF.

      This function is not typically called by an end user. These calls will
      occur during the evaluation of the dataframe.



   .. py:method:: __repr__() -> str

      Print a string representation of the Scalar UDF.



   .. py:method:: from_pycapsule(func: ScalarUDFExportable) -> ScalarUDF
      :staticmethod:


      Create a Scalar UDF from ScalarUDF PyCapsule object.

      This function will instantiate a Scalar UDF that uses a DataFusion
      ScalarUDF that is exported via the FFI bindings.



   .. py:method:: udf(input_fields: collections.abc.Sequence[pyarrow.DataType | pyarrow.Field] | pyarrow.DataType | pyarrow.Field, return_field: pyarrow.DataType | pyarrow.Field, volatility: Volatility | str, name: str | None = None) -> collections.abc.Callable[Ellipsis, ScalarUDF]
                  udf(func: collections.abc.Callable[Ellipsis, _R], input_fields: collections.abc.Sequence[pyarrow.DataType | pyarrow.Field] | pyarrow.DataType | pyarrow.Field, return_field: pyarrow.DataType | pyarrow.Field, volatility: Volatility | str, name: str | None = None) -> ScalarUDF
                  udf(func: ScalarUDFExportable) -> ScalarUDF
      :staticmethod:


      Create a new User-Defined Function (UDF).

      This class can be used both as either a function or a decorator.

      Usage:
          - As a function: ``udf(func, input_fields, return_field, volatility, name)``.
          - As a decorator: ``@udf(input_fields, return_field, volatility, name)``.
            When used a decorator, do **not** pass ``func`` explicitly.

      In lieu of passing a PyArrow Field, you can pass a DataType for simplicity.
      When you do so, it will be assumed that the nullability of the inputs and
      output are True and that they have no metadata.

      :param func: Only needed when calling as a function.
                   Skip this argument when using `udf` as a decorator. If you have a Rust
                   backed ScalarUDF within a PyCapsule, you can pass this parameter
                   and ignore the rest. They will be determined directly from the
                   underlying function. See the online documentation for more information.
      :type func: Callable, optional
      :param input_fields: The data types or Fields
                           of the arguments to ``func``. This list must be of the same length
                           as the number of arguments.
      :type input_fields: list[pa.Field | pa.DataType]
      :param return_field: The field of the return value from the function.
      :type return_field: _R
      :param volatility: See `Volatility` for allowed values.
      :type volatility: Volatility | str
      :param name: A descriptive name for the function.
      :type name: Optional[str]

      :returns: A user-defined function that can be used in SQL expressions,
                data aggregation, or window function calls.

      Example: Using ``udf`` as a function::

          def double_func(x):
              return x * 2
          double_udf = udf(double_func, [pa.int32()], pa.int32(),
          "volatile", "double_it")

      Example: Using ``udf`` as a decorator::

          @udf([pa.int32()], pa.int32(), "volatile", "double_it")
          def double_udf(x):
              return x * 2



   .. py:attribute:: _udf


.. py:class:: SessionConfig(config_options: dict[str, str] | None = None)

   Session configuration options.

   Create a new :py:class:`SessionConfig` with the given configuration options.

   :param config_options: Configuration options.


   .. py:method:: set(key: str, value: str) -> SessionConfig

      Set a configuration option.

      Args:
      key: Option key.
      value: Option value.

      :returns: A new :py:class:`SessionConfig` object with the updated setting.



   .. py:method:: with_batch_size(batch_size: int) -> SessionConfig

      Customize batch size.

      :param batch_size: Batch size.

      :returns: A new :py:class:`SessionConfig` object with the updated setting.



   .. py:method:: with_create_default_catalog_and_schema(enabled: bool = True) -> SessionConfig

      Control if the default catalog and schema will be automatically created.

      :param enabled: Whether the default catalog and schema will be
                      automatically created.

      :returns: A new :py:class:`SessionConfig` object with the updated setting.



   .. py:method:: with_default_catalog_and_schema(catalog: str, schema: str) -> SessionConfig

      Select a name for the default catalog and schema.

      :param catalog: Catalog name.
      :param schema: Schema name.

      :returns: A new :py:class:`SessionConfig` object with the updated setting.



   .. py:method:: with_information_schema(enabled: bool = True) -> SessionConfig

      Enable or disable the inclusion of ``information_schema`` virtual tables.

      :param enabled: Whether to include ``information_schema`` virtual tables.

      :returns: A new :py:class:`SessionConfig` object with the updated setting.



   .. py:method:: with_parquet_pruning(enabled: bool = True) -> SessionConfig

      Enable or disable the use of pruning predicate for parquet readers.

      Pruning predicates will enable the reader to skip row groups.

      :param enabled: Whether to use pruning predicate for parquet readers.

      :returns: A new :py:class:`SessionConfig` object with the updated setting.



   .. py:method:: with_repartition_aggregations(enabled: bool = True) -> SessionConfig

      Enable or disable the use of repartitioning for aggregations.

      Enabling this improves parallelism.

      :param enabled: Whether to use repartitioning for aggregations.

      :returns: A new :py:class:`SessionConfig` object with the updated setting.



   .. py:method:: with_repartition_file_min_size(size: int) -> SessionConfig

      Set minimum file range size for repartitioning scans.

      :param size: Minimum file range size.

      :returns: A new :py:class:`SessionConfig` object with the updated setting.



   .. py:method:: with_repartition_file_scans(enabled: bool = True) -> SessionConfig

      Enable or disable the use of repartitioning for file scans.

      :param enabled: Whether to use repartitioning for file scans.

      :returns: A new :py:class:`SessionConfig` object with the updated setting.



   .. py:method:: with_repartition_joins(enabled: bool = True) -> SessionConfig

      Enable or disable the use of repartitioning for joins to improve parallelism.

      :param enabled: Whether to use repartitioning for joins.

      :returns: A new :py:class:`SessionConfig` object with the updated setting.



   .. py:method:: with_repartition_sorts(enabled: bool = True) -> SessionConfig

      Enable or disable the use of repartitioning for window functions.

      This may improve parallelism.

      :param enabled: Whether to use repartitioning for window functions.

      :returns: A new :py:class:`SessionConfig` object with the updated setting.



   .. py:method:: with_repartition_windows(enabled: bool = True) -> SessionConfig

      Enable or disable the use of repartitioning for window functions.

      This may improve parallelism.

      :param enabled: Whether to use repartitioning for window functions.

      :returns: A new :py:class:`SessionConfig` object with the updated setting.



   .. py:method:: with_target_partitions(target_partitions: int) -> SessionConfig

      Customize the number of target partitions for query execution.

      Increasing partitions can increase concurrency.

      :param target_partitions: Number of target partitions.

      :returns: A new :py:class:`SessionConfig` object with the updated setting.



   .. py:attribute:: config_internal


.. py:class:: Table(table: Table | datafusion.context.TableProviderExportable | datafusion.DataFrame | pyarrow.dataset.Dataset, ctx: datafusion.SessionContext | None = None)

   A DataFusion table.

   Internally we currently support the following types of tables:

   - Tables created using built-in DataFusion methods, such as
     reading from CSV or Parquet
   - pyarrow datasets
   - DataFusion DataFrames, which will be converted into a view
   - Externally provided tables implemented with the FFI PyCapsule
     interface (advanced)

   Constructor.


   .. py:method:: __repr__() -> str

      Print a string representation of the table.



   .. py:method:: from_dataset(dataset: pyarrow.dataset.Dataset) -> Table
      :staticmethod:


      Turn a :mod:`pyarrow.dataset` ``Dataset`` into a :class:`Table`.



   .. py:attribute:: __slots__
      :value: ('_inner',)



   .. py:attribute:: _inner


   .. py:property:: kind
      :type: str


      Returns the kind of table.


   .. py:property:: schema
      :type: pyarrow.Schema


      Returns the schema associated with this table.


.. py:class:: TableFunction(name: str, func: collections.abc.Callable[[], any], ctx: datafusion.SessionContext | None = None)

   Class for performing user-defined table functions (UDTF).

   Table functions generate new table providers based on the
   input expressions.

   Instantiate a user-defined table function (UDTF).

   See :py:func:`udtf` for a convenience function and argument
   descriptions.


   .. py:method:: __call__(*args: datafusion.expr.Expr) -> Any

      Execute the UDTF and return a table provider.



   .. py:method:: __repr__() -> str

      User printable representation.



   .. py:method:: _create_table_udf(func: collections.abc.Callable[Ellipsis, Any], name: str) -> TableFunction
      :staticmethod:


      Create a TableFunction instance from function arguments.



   .. py:method:: _create_table_udf_decorator(name: str | None = None) -> collections.abc.Callable[[collections.abc.Callable[[], WindowEvaluator]], collections.abc.Callable[Ellipsis, datafusion.expr.Expr]]
      :staticmethod:


      Create a decorator for a WindowUDF.



   .. py:method:: udtf(name: str) -> collections.abc.Callable[Ellipsis, Any]
                  udtf(func: collections.abc.Callable[[], Any], name: str) -> TableFunction
      :staticmethod:


      Create a new User-Defined Table Function (UDTF).



   .. py:attribute:: _udtf


.. py:class:: WindowFrame(units: str, start_bound: Any | None, end_bound: Any | None)

   Defines a window frame for performing window operations.

   Construct a window frame using the given parameters.

   :param units: Should be one of ``rows``, ``range``, or ``groups``.
   :param start_bound: Sets the preceding bound. Must be >= 0. If none, this
                       will be set to unbounded. If unit type is ``groups``, this
                       parameter must be set.
   :param end_bound: Sets the following bound. Must be >= 0. If none, this
                     will be set to unbounded. If unit type is ``groups``, this
                     parameter must be set.


   .. py:method:: __repr__() -> str

      Print a string representation of the window frame.



   .. py:method:: get_frame_units() -> str

      Returns the window frame units for the bounds.



   .. py:method:: get_lower_bound() -> WindowFrameBound

      Returns starting bound.



   .. py:method:: get_upper_bound() -> WindowFrameBound

      Returns end bound.



   .. py:attribute:: window_frame


.. py:class:: WindowUDF(name: str, func: collections.abc.Callable[[], WindowEvaluator], input_types: list[pyarrow.DataType], return_type: pyarrow.DataType, volatility: Volatility | str)

   Class for performing window user-defined functions (UDF).

   Window UDFs operate on a partition of rows. See
   also :py:class:`ScalarUDF` for operating on a row by row basis.

   Instantiate a user-defined window function (UDWF).

   See :py:func:`udwf` for a convenience function and argument
   descriptions.


   .. py:method:: __call__(*args: datafusion.expr.Expr) -> datafusion.expr.Expr

      Execute the UDWF.

      This function is not typically called by an end user. These calls will
      occur during the evaluation of the dataframe.



   .. py:method:: __repr__() -> str

      Print a string representation of the Window UDF.



   .. py:method:: _create_window_udf(func: collections.abc.Callable[[], WindowEvaluator], input_types: pyarrow.DataType | list[pyarrow.DataType], return_type: pyarrow.DataType, volatility: Volatility | str, name: str | None = None) -> WindowUDF
      :staticmethod:


      Create a WindowUDF instance from function arguments.



   .. py:method:: _create_window_udf_decorator(input_types: pyarrow.DataType | list[pyarrow.DataType], return_type: pyarrow.DataType, volatility: Volatility | str, name: str | None = None) -> collections.abc.Callable[[collections.abc.Callable[[], WindowEvaluator]], collections.abc.Callable[Ellipsis, datafusion.expr.Expr]]
      :staticmethod:


      Create a decorator for a WindowUDF.



   .. py:method:: _get_default_name(func: collections.abc.Callable) -> str
      :staticmethod:


      Get the default name for a function based on its attributes.



   .. py:method:: _normalize_input_types(input_types: pyarrow.DataType | list[pyarrow.DataType]) -> list[pyarrow.DataType]
      :staticmethod:


      Convert a single DataType to a list if needed.



   .. py:method:: from_pycapsule(func: WindowUDFExportable) -> WindowUDF
      :staticmethod:


      Create a Window UDF from WindowUDF PyCapsule object.

      This function will instantiate a Window UDF that uses a DataFusion
      WindowUDF that is exported via the FFI bindings.



   .. py:method:: udwf(input_types: pyarrow.DataType | list[pyarrow.DataType], return_type: pyarrow.DataType, volatility: Volatility | str, name: str | None = None) -> collections.abc.Callable[Ellipsis, WindowUDF]
                  udwf(func: collections.abc.Callable[[], WindowEvaluator], input_types: pyarrow.DataType | list[pyarrow.DataType], return_type: pyarrow.DataType, volatility: Volatility | str, name: str | None = None) -> WindowUDF
      :staticmethod:


      Create a new User-Defined Window Function (UDWF).

      This class can be used both as either a function or a decorator.

      Usage:
          - As a function: ``udwf(func, input_types, return_type, volatility, name)``.
          - As a decorator: ``@udwf(input_types, return_type, volatility, name)``.
            When using ``udwf`` as a decorator, do not pass ``func`` explicitly.

      Function example::

          import pyarrow as pa

          class BiasedNumbers(WindowEvaluator):
              def __init__(self, start: int = 0) -> None:
                  self.start = start

              def evaluate_all(self, values: list[pa.Array],
                  num_rows: int) -> pa.Array:
                  return pa.array([self.start + i for i in range(num_rows)])

          def bias_10() -> BiasedNumbers:
              return BiasedNumbers(10)

          udwf1 = udwf(BiasedNumbers, pa.int64(), pa.int64(), "immutable")
          udwf2 = udwf(bias_10, pa.int64(), pa.int64(), "immutable")
          udwf3 = udwf(lambda: BiasedNumbers(20), pa.int64(), pa.int64(), "immutable")


      Decorator example::

          @udwf(pa.int64(), pa.int64(), "immutable")
          def biased_numbers() -> BiasedNumbers:
              return BiasedNumbers(10)

      :param func: Only needed when calling as a function. Skip this argument when
                   using ``udwf`` as a decorator. If you have a Rust backed WindowUDF
                   within a PyCapsule, you can pass this parameter and ignore the rest.
                   They will be determined directly from the underlying function. See
                   the online documentation for more information.
      :param input_types: The data types of the arguments.
      :param return_type: The data type of the return value.
      :param volatility: See :py:class:`Volatility` for allowed values.
      :param name: A descriptive name for the function.

      :returns: A user-defined window function that can be used in window function calls.



   .. py:attribute:: _udwf


.. py:function:: configure_formatter(**kwargs: Any) -> None

   Configure the global DataFrame HTML formatter.

   This function creates a new formatter with the provided configuration
   and sets it as the global formatter for all DataFrames.

   :param \*\*kwargs: Formatter configuration parameters like max_cell_length,
                      max_width, max_height, enable_cell_expansion, etc.

   :raises ValueError: If any invalid parameters are provided

   .. rubric:: Example

   >>> from datafusion.html_formatter import configure_formatter
   >>> configure_formatter(
   ...     max_cell_length=50,
   ...     max_height=500,
   ...     enable_cell_expansion=True,
   ...     use_shared_styles=True
   ... )


.. py:function:: lit(value: Any) -> expr.Expr

   Create a literal expression.


.. py:function:: literal(value: Any) -> expr.Expr

   Create a literal expression.


.. py:function:: read_avro(path: str | pathlib.Path, schema: pyarrow.Schema | None = None, file_partition_cols: list[tuple[str, str | pyarrow.DataType]] | None = None, file_extension: str = '.avro') -> datafusion.dataframe.DataFrame

   Create a :py:class:`DataFrame` for reading Avro data source.

   This function will use the global context. Any functions or tables registered
   with another context may not be accessible when used with a DataFrame created
   using this function.

   :param path: Path to the Avro file.
   :param schema: The data source schema.
   :param file_partition_cols: Partition columns.
   :param file_extension: File extension to select.

   :returns: DataFrame representation of the read Avro file


.. py:function:: read_csv(path: str | pathlib.Path | list[str] | list[pathlib.Path], schema: pyarrow.Schema | None = None, has_header: bool = True, delimiter: str = ',', schema_infer_max_records: int = 1000, file_extension: str = '.csv', table_partition_cols: list[tuple[str, str | pyarrow.DataType]] | None = None, file_compression_type: str | None = None, options: datafusion.options.CsvReadOptions | None = None) -> datafusion.dataframe.DataFrame

   Read a CSV data source.

   This function will use the global context. Any functions or tables registered
   with another context may not be accessible when used with a DataFrame created
   using this function.

   :param path: Path to the CSV file
   :param schema: An optional schema representing the CSV files. If None, the
                  CSV reader will try to infer it based on data in file.
   :param has_header: Whether the CSV file have a header. If schema inference
                      is run on a file with no headers, default column names are
                      created.
   :param delimiter: An optional column delimiter.
   :param schema_infer_max_records: Maximum number of rows to read from CSV
                                    files for schema inference if needed.
   :param file_extension: File extension; only files with this extension are
                          selected for data input.
   :param table_partition_cols: Partition columns.
   :param file_compression_type: File compression type.
   :param options: Set advanced options for CSV reading. This cannot be
                   combined with any of the other options in this method.

   :returns: DataFrame representation of the read CSV files


.. py:function:: read_json(path: str | pathlib.Path, schema: pyarrow.Schema | None = None, schema_infer_max_records: int = 1000, file_extension: str = '.json', table_partition_cols: list[tuple[str, str | pyarrow.DataType]] | None = None, file_compression_type: str | None = None) -> datafusion.dataframe.DataFrame

   Read a line-delimited JSON data source.

   This function will use the global context. Any functions or tables registered
   with another context may not be accessible when used with a DataFrame created
   using this function.

   :param path: Path to the JSON file.
   :param schema: The data source schema.
   :param schema_infer_max_records: Maximum number of rows to read from JSON
                                    files for schema inference if needed.
   :param file_extension: File extension; only files with this extension are
                          selected for data input.
   :param table_partition_cols: Partition columns.
   :param file_compression_type: File compression type.

   :returns: DataFrame representation of the read JSON files.


.. py:function:: read_parquet(path: str | pathlib.Path, table_partition_cols: list[tuple[str, str | pyarrow.DataType]] | None = None, parquet_pruning: bool = True, file_extension: str = '.parquet', skip_metadata: bool = True, schema: pyarrow.Schema | None = None, file_sort_order: list[list[datafusion.expr.Expr]] | None = None) -> datafusion.dataframe.DataFrame

   Read a Parquet source into a :py:class:`~datafusion.dataframe.Dataframe`.

   This function will use the global context. Any functions or tables registered
   with another context may not be accessible when used with a DataFrame created
   using this function.

   :param path: Path to the Parquet file.
   :param table_partition_cols: Partition columns.
   :param parquet_pruning: Whether the parquet reader should use the predicate
                           to prune row groups.
   :param file_extension: File extension; only files with this extension are
                          selected for data input.
   :param skip_metadata: Whether the parquet reader should skip any metadata
                         that may be in the file schema. This can help avoid schema
                         conflicts due to metadata.
   :param schema: An optional schema representing the parquet files. If None,
                  the parquet reader will try to infer it based on data in the
                  file.
   :param file_sort_order: Sort order for the file.

   :returns: DataFrame representation of the read Parquet files


.. py:data:: DFSchema

.. py:data:: col
   :type:  Col

.. py:data:: column
   :type:  Col

.. py:data:: udaf

.. py:data:: udf

.. py:data:: udtf

.. py:data:: udwf

