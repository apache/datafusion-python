datafusion.io
=============

.. py:module:: datafusion.io

.. autoapi-nested-parse::

   IO read functions using global context.



Functions
---------

.. autoapisummary::

   datafusion.io.read_avro
   datafusion.io.read_csv
   datafusion.io.read_json
   datafusion.io.read_parquet


Module Contents
---------------

.. py:function:: read_avro(path: str | pathlib.Path, schema: pyarrow.Schema | None = None, file_partition_cols: list[tuple[str, str | pyarrow.DataType]] | None = None, file_extension: str = '.avro') -> datafusion.dataframe.DataFrame

   Create a :py:class:`DataFrame` for reading Avro data source.

   This function will use the global context. Any functions or tables registered
   with another context may not be accessible when used with a DataFrame created
   using this function.

   :param path: Path to the Avro file.
   :param schema: The data source schema.
   :param file_partition_cols: Partition columns.
   :param file_extension: File extension to select.

   :returns: DataFrame representation of the read Avro file


.. py:function:: read_csv(path: str | pathlib.Path | list[str] | list[pathlib.Path], schema: pyarrow.Schema | None = None, has_header: bool = True, delimiter: str = ',', schema_infer_max_records: int = 1000, file_extension: str = '.csv', table_partition_cols: list[tuple[str, str | pyarrow.DataType]] | None = None, file_compression_type: str | None = None, options: datafusion.options.CsvReadOptions | None = None) -> datafusion.dataframe.DataFrame

   Read a CSV data source.

   This function will use the global context. Any functions or tables registered
   with another context may not be accessible when used with a DataFrame created
   using this function.

   :param path: Path to the CSV file
   :param schema: An optional schema representing the CSV files. If None, the
                  CSV reader will try to infer it based on data in file.
   :param has_header: Whether the CSV file have a header. If schema inference
                      is run on a file with no headers, default column names are
                      created.
   :param delimiter: An optional column delimiter.
   :param schema_infer_max_records: Maximum number of rows to read from CSV
                                    files for schema inference if needed.
   :param file_extension: File extension; only files with this extension are
                          selected for data input.
   :param table_partition_cols: Partition columns.
   :param file_compression_type: File compression type.
   :param options: Set advanced options for CSV reading. This cannot be
                   combined with any of the other options in this method.

   :returns: DataFrame representation of the read CSV files


.. py:function:: read_json(path: str | pathlib.Path, schema: pyarrow.Schema | None = None, schema_infer_max_records: int = 1000, file_extension: str = '.json', table_partition_cols: list[tuple[str, str | pyarrow.DataType]] | None = None, file_compression_type: str | None = None) -> datafusion.dataframe.DataFrame

   Read a line-delimited JSON data source.

   This function will use the global context. Any functions or tables registered
   with another context may not be accessible when used with a DataFrame created
   using this function.

   :param path: Path to the JSON file.
   :param schema: The data source schema.
   :param schema_infer_max_records: Maximum number of rows to read from JSON
                                    files for schema inference if needed.
   :param file_extension: File extension; only files with this extension are
                          selected for data input.
   :param table_partition_cols: Partition columns.
   :param file_compression_type: File compression type.

   :returns: DataFrame representation of the read JSON files.


.. py:function:: read_parquet(path: str | pathlib.Path, table_partition_cols: list[tuple[str, str | pyarrow.DataType]] | None = None, parquet_pruning: bool = True, file_extension: str = '.parquet', skip_metadata: bool = True, schema: pyarrow.Schema | None = None, file_sort_order: list[list[datafusion.expr.Expr]] | None = None) -> datafusion.dataframe.DataFrame

   Read a Parquet source into a :py:class:`~datafusion.dataframe.Dataframe`.

   This function will use the global context. Any functions or tables registered
   with another context may not be accessible when used with a DataFrame created
   using this function.

   :param path: Path to the Parquet file.
   :param table_partition_cols: Partition columns.
   :param parquet_pruning: Whether the parquet reader should use the predicate
                           to prune row groups.
   :param file_extension: File extension; only files with this extension are
                          selected for data input.
   :param skip_metadata: Whether the parquet reader should skip any metadata
                         that may be in the file schema. This can help avoid schema
                         conflicts due to metadata.
   :param schema: An optional schema representing the parquet files. If None,
                  the parquet reader will try to infer it based on data in the
                  file.
   :param file_sort_order: Sort order for the file.

   :returns: DataFrame representation of the read Parquet files


