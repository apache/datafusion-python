datafusion.dataframe
====================

.. py:module:: datafusion.dataframe

.. autoapi-nested-parse::

   :py:class:`DataFrame` is one of the core concepts in DataFusion.

   See :ref:`user_guide_concepts` in the online documentation for more information.



Classes
-------

.. autoapisummary::

   datafusion.dataframe.Compression
   datafusion.dataframe.DataFrame


Module Contents
---------------

.. py:class:: Compression(*args, **kwds)

   Bases: :py:obj:`enum.Enum`


   Enum representing the available compression types for Parquet files.


   .. py:method:: from_str(value: str) -> Compression
      :classmethod:


      Convert a string to a Compression enum value.

      :param value: The string representation of the compression type.

      :returns: The Compression enum lowercase value.

      :raises ValueError: If the string does not match any Compression enum value.



   .. py:method:: get_default_level() -> Optional[int]

      Get the default compression level for the compression type.

      :returns: The default compression level for the compression type.



   .. py:attribute:: BROTLI
      :value: 'brotli'



   .. py:attribute:: GZIP
      :value: 'gzip'



   .. py:attribute:: LZ4
      :value: 'lz4'



   .. py:attribute:: LZ4_RAW
      :value: 'lz4_raw'



   .. py:attribute:: SNAPPY
      :value: 'snappy'



   .. py:attribute:: UNCOMPRESSED
      :value: 'uncompressed'



   .. py:attribute:: ZSTD
      :value: 'zstd'



.. py:class:: DataFrame(df: datafusion._internal.DataFrame)

   Two dimensional table representation of data.

   See :ref:`user_guide_concepts` in the online documentation for more information.

   This constructor is not to be used by the end user.

   See :py:class:`~datafusion.context.SessionContext` for methods to
   create a :py:class:`DataFrame`.


   .. py:method:: __arrow_c_stream__(requested_schema: pyarrow.Schema) -> Any

      Export an Arrow PyCapsule Stream.

      This will execute and collect the DataFrame. We will attempt to respect the
      requested schema, but only trivial transformations will be applied such as only
      returning the fields listed in the requested schema if their data types match
      those in the DataFrame.

      :param requested_schema: Attempt to provide the DataFrame using this schema.

      :returns: Arrow PyCapsule object.



   .. py:method:: __getitem__(key: str | list[str]) -> DataFrame

      Return a new :py:class`DataFrame` with the specified column or columns.

      :param key: Column name or list of column names to select.

      :returns: DataFrame with the specified column or columns.



   .. py:method:: __repr__() -> str

      Return a string representation of the DataFrame.

      :returns: String representation of the DataFrame.



   .. py:method:: _repr_html_() -> str


   .. py:method:: aggregate(group_by: list[datafusion.expr.Expr] | datafusion.expr.Expr, aggs: list[datafusion.expr.Expr] | datafusion.expr.Expr) -> DataFrame

      Aggregates the rows of the current DataFrame.

      :param group_by: List of expressions to group by.
      :param aggs: List of expressions to aggregate.

      :returns: DataFrame after aggregation.



   .. py:method:: cache() -> DataFrame

      Cache the DataFrame as a memory table.

      :returns: Cached DataFrame.



   .. py:method:: cast(mapping: dict[str, pyarrow.DataType[Any]]) -> DataFrame

      Cast one or more columns to a different data type.

      :param mapping: Mapped with column as key and column dtype as value.

      :returns: DataFrame after casting columns



   .. py:method:: collect() -> list[pyarrow.RecordBatch]

      Execute this :py:class:`DataFrame` and collect results into memory.

      Prior to calling ``collect``, modifying a DataFrme simply updates a plan
      (no actual computation is performed). Calling ``collect`` triggers the
      computation.

      :returns: List of :py:class:`pyarrow.RecordBatch` collected from the DataFrame.



   .. py:method:: collect_partitioned() -> list[list[pyarrow.RecordBatch]]

      Execute this DataFrame and collect all partitioned results.

      This operation returns :py:class:`pyarrow.RecordBatch` maintaining the input
      partitioning.

      :returns:

                List of list of :py:class:`RecordBatch` collected from the
                    DataFrame.



   .. py:method:: count() -> int

      Return the total number of rows in this :py:class:`DataFrame`.

      Note that this method will actually run a plan to calculate the
      count, which may be slow for large or complicated DataFrames.

      :returns: Number of rows in the DataFrame.



   .. py:method:: describe() -> DataFrame

      Return the statistics for this DataFrame.

      Only summarized numeric datatypes at the moments and returns nulls
      for non-numeric datatypes.

      The output format is modeled after pandas.

      :returns: A summary DataFrame containing statistics.



   .. py:method:: distinct() -> DataFrame

      Return a new :py:class:`DataFrame` with all duplicated rows removed.

      :returns: DataFrame after removing duplicates.



   .. py:method:: drop(*columns: str) -> DataFrame

      Drop arbitrary amount of columns.

      :param columns: Column names to drop from the dataframe.

      :returns: DataFrame with those columns removed in the projection.



   .. py:method:: except_all(other: DataFrame) -> DataFrame

      Calculate the exception of two :py:class:`DataFrame`.

      The two :py:class:`DataFrame` must have exactly the same schema.

      :param other: DataFrame to calculate exception with.

      :returns: DataFrame after exception.



   .. py:method:: execute_stream() -> datafusion.record_batch.RecordBatchStream

      Executes this DataFrame and returns a stream over a single partition.

      :returns: Record Batch Stream over a single partition.



   .. py:method:: execute_stream_partitioned() -> list[datafusion.record_batch.RecordBatchStream]

      Executes this DataFrame and returns a stream for each partition.

      :returns: One record batch stream per partition.



   .. py:method:: execution_plan() -> datafusion.plan.ExecutionPlan

      Return the execution/physical plan.

      :returns: Execution plan.



   .. py:method:: explain(verbose: bool = False, analyze: bool = False) -> DataFrame

      Return a DataFrame with the explanation of its plan so far.

      If ``analyze`` is specified, runs the plan and reports metrics.

      :param verbose: If ``True``, more details will be included.
      :param analyze: If ``Tru`e``, the plan will run and metrics reported.

      :returns: DataFrame with the explanation of its plan.



   .. py:method:: fill_null(value: Any, subset: list[str] | None = None) -> DataFrame

      Fill null values in specified columns with a value.

      :param value: Value to replace nulls with. Will be cast to match column type.
      :param subset: Optional list of column names to fill. If None, fills all columns.

      :returns: DataFrame with null values replaced where type casting is possible

      .. rubric:: Examples

      >>> df = df.fill_null(0)  # Fill all nulls with 0 where possible
      >>> # Fill nulls in specific string columns
      >>> df = df.fill_null("missing", subset=["name", "category"])

      .. rubric:: Notes

      - Only fills nulls in columns where the value can be cast to the column type
      - For columns where casting fails, the original column is kept unchanged
      - For columns not in subset, the original column is kept unchanged



   .. py:method:: filter(*predicates: datafusion.expr.Expr) -> DataFrame

      Return a DataFrame for which ``predicate`` evaluates to ``True``.

      Rows for which ``predicate`` evaluates to ``False`` or ``None`` are filtered
      out.  If more than one predicate is provided, these predicates will be
      combined as a logical AND. If more complex logic is required, see the
      logical operations in :py:mod:`~datafusion.functions`.

      :param predicates: Predicate expression(s) to filter the DataFrame.

      :returns: DataFrame after filtering.



   .. py:method:: head(n: int = 5) -> DataFrame

      Return a new :py:class:`DataFrame` with a limited number of rows.

      :param n: Number of rows to take from the head of the DataFrame.

      :returns: DataFrame after limiting.



   .. py:method:: intersect(other: DataFrame) -> DataFrame

      Calculate the intersection of two :py:class:`DataFrame`.

      The two :py:class:`DataFrame` must have exactly the same schema.

      :param other: DataFrame to intersect with.

      :returns: DataFrame after intersection.



   .. py:method:: into_view() -> pyarrow.Table

      Convert DataFrame as a ViewTable which can be used in register_table.



   .. py:method:: join(right: DataFrame, on: str | Sequence[str], how: Literal['inner', 'left', 'right', 'full', 'semi', 'anti'] = 'inner', *, left_on: None = None, right_on: None = None, join_keys: None = None) -> DataFrame
                  join(right: DataFrame, on: None = None, how: Literal['inner', 'left', 'right', 'full', 'semi', 'anti'] = 'inner', *, left_on: str | Sequence[str], right_on: str | Sequence[str], join_keys: tuple[list[str], list[str]] | None = None) -> DataFrame
                  join(right: DataFrame, on: None = None, how: Literal['inner', 'left', 'right', 'full', 'semi', 'anti'] = 'inner', *, join_keys: tuple[list[str], list[str]], left_on: None = None, right_on: None = None) -> DataFrame

      Join this :py:class:`DataFrame` with another :py:class:`DataFrame`.

      `on` has to be provided or both `left_on` and `right_on` in conjunction.

      :param right: Other DataFrame to join with.
      :param on: Column names to join on in both dataframes.
      :param how: Type of join to perform. Supported types are "inner", "left",
                  "right", "full", "semi", "anti".
      :param left_on: Join column of the left dataframe.
      :param right_on: Join column of the right dataframe.
      :param join_keys: Tuple of two lists of column names to join on. [Deprecated]

      :returns: DataFrame after join.



   .. py:method:: join_on(right: DataFrame, *on_exprs: datafusion.expr.Expr, how: Literal['inner', 'left', 'right', 'full', 'semi', 'anti'] = 'inner') -> DataFrame

      Join two :py:class:`DataFrame` using the specified expressions.

      On expressions are used to support in-equality predicates. Equality
      predicates are correctly optimized

      :param right: Other DataFrame to join with.
      :param on_exprs: single or multiple (in)-equality predicates.
      :param how: Type of join to perform. Supported types are "inner", "left",
                  "right", "full", "semi", "anti".

      :returns: DataFrame after join.



   .. py:method:: limit(count: int, offset: int = 0) -> DataFrame

      Return a new :py:class:`DataFrame` with a limited number of rows.

      :param count: Number of rows to limit the DataFrame to.
      :param offset: Number of rows to skip.

      :returns: DataFrame after limiting.



   .. py:method:: logical_plan() -> datafusion.plan.LogicalPlan

      Return the unoptimized ``LogicalPlan``.

      :returns: Unoptimized logical plan.



   .. py:method:: optimized_logical_plan() -> datafusion.plan.LogicalPlan

      Return the optimized ``LogicalPlan``.

      :returns: Optimized logical plan.



   .. py:method:: repartition(num: int) -> DataFrame

      Repartition a DataFrame into ``num`` partitions.

      The batches allocation uses a round-robin algorithm.

      :param num: Number of partitions to repartition the DataFrame into.

      :returns: Repartitioned DataFrame.



   .. py:method:: repartition_by_hash(*exprs: datafusion.expr.Expr, num: int) -> DataFrame

      Repartition a DataFrame using a hash partitioning scheme.

      :param exprs: Expressions to evaluate and perform hashing on.
      :param num: Number of partitions to repartition the DataFrame into.

      :returns: Repartitioned DataFrame.



   .. py:method:: schema() -> pyarrow.Schema

      Return the :py:class:`pyarrow.Schema` of this DataFrame.

      The output schema contains information on the name, data type, and
      nullability for each column.

      :returns: Describing schema of the DataFrame



   .. py:method:: select(*exprs: datafusion.expr.Expr | str) -> DataFrame

      Project arbitrary expressions into a new :py:class:`DataFrame`.

      :param exprs: Either column names or :py:class:`~datafusion.expr.Expr` to select.

      :returns: DataFrame after projection. It has one column for each expression.

      Example usage:

      The following example will return 3 columns from the original dataframe.
      The first two columns will be the original column ``a`` and ``b`` since the
      string "a" is assumed to refer to column selection. Also a duplicate of
      column ``a`` will be returned with the column name ``alternate_a``::

          df = df.select("a", col("b"), col("a").alias("alternate_a"))




   .. py:method:: select_columns(*args: str) -> DataFrame

      Filter the DataFrame by columns.

      :returns: DataFrame only containing the specified columns.



   .. py:method:: show(num: int = 20) -> None

      Execute the DataFrame and print the result to the console.

      :param num: Number of lines to show.



   .. py:method:: sort(*exprs: datafusion.expr.Expr | datafusion.expr.SortExpr) -> DataFrame

      Sort the DataFrame by the specified sorting expressions.

      Note that any expression can be turned into a sort expression by
      calling its` ``sort`` method.

      :param exprs: Sort expressions, applied in order.

      :returns: DataFrame after sorting.



   .. py:method:: tail(n: int = 5) -> DataFrame

      Return a new :py:class:`DataFrame` with a limited number of rows.

      Be aware this could be potentially expensive since the row size needs to be
      determined of the dataframe. This is done by collecting it.

      :param n: Number of rows to take from the tail of the DataFrame.

      :returns: DataFrame after limiting.



   .. py:method:: to_arrow_table() -> pyarrow.Table

      Execute the :py:class:`DataFrame` and convert it into an Arrow Table.

      :returns: Arrow Table.



   .. py:method:: to_pandas() -> pandas.DataFrame

      Execute the :py:class:`DataFrame` and convert it into a Pandas DataFrame.

      :returns: Pandas DataFrame.



   .. py:method:: to_polars() -> polars.DataFrame

      Execute the :py:class:`DataFrame` and convert it into a Polars DataFrame.

      :returns: Polars DataFrame.



   .. py:method:: to_pydict() -> dict[str, list[Any]]

      Execute the :py:class:`DataFrame` and convert it into a dictionary of lists.

      :returns: Dictionary of lists.



   .. py:method:: to_pylist() -> list[dict[str, Any]]

      Execute the :py:class:`DataFrame` and convert it into a list of dictionaries.

      :returns: List of dictionaries.



   .. py:method:: transform(func: Callable[Ellipsis, DataFrame], *args: Any) -> DataFrame

      Apply a function to the current DataFrame which returns another DataFrame.

      This is useful for chaining together multiple functions. For example::

          def add_3(df: DataFrame) -> DataFrame:
              return df.with_column("modified", lit(3))

          def within_limit(df: DataFrame, limit: int) -> DataFrame:
              return df.filter(col("a") < lit(limit)).distinct()

          df = df.transform(modify_df).transform(within_limit, 4)

      :param func: A callable function that takes a DataFrame as it's first argument
      :param args: Zero or more arguments to pass to `func`

      :returns: After applying func to the original dataframe.
      :rtype: DataFrame



   .. py:method:: union(other: DataFrame, distinct: bool = False) -> DataFrame

      Calculate the union of two :py:class:`DataFrame`.

      The two :py:class:`DataFrame` must have exactly the same schema.

      :param other: DataFrame to union with.
      :param distinct: If ``True``, duplicate rows will be removed.

      :returns: DataFrame after union.



   .. py:method:: union_distinct(other: DataFrame) -> DataFrame

      Calculate the distinct union of two :py:class:`DataFrame`.

      The two :py:class:`DataFrame` must have exactly the same schema.
      Any duplicate rows are discarded.

      :param other: DataFrame to union with.

      :returns: DataFrame after union.



   .. py:method:: unnest_columns(*columns: str, preserve_nulls: bool = True) -> DataFrame

      Expand columns of arrays into a single row per array element.

      :param columns: Column names to perform unnest operation on.
      :param preserve_nulls: If False, rows with null entries will not be
                             returned.

      :returns: A DataFrame with the columns expanded.



   .. py:method:: with_column(name: str, expr: datafusion.expr.Expr) -> DataFrame

      Add an additional column to the DataFrame.

      :param name: Name of the column to add.
      :param expr: Expression to compute the column.

      :returns: DataFrame with the new column.



   .. py:method:: with_column_renamed(old_name: str, new_name: str) -> DataFrame

      Rename one column by applying a new projection.

      This is a no-op if the column to be renamed does not exist.

      The method supports case sensitive rename with wrapping column name
      into one the following symbols (" or ' or \`).

      :param old_name: Old column name.
      :param new_name: New column name.

      :returns: DataFrame with the column renamed.



   .. py:method:: with_columns(*exprs: datafusion.expr.Expr | Iterable[datafusion.expr.Expr], **named_exprs: datafusion.expr.Expr) -> DataFrame

      Add columns to the DataFrame.

      By passing expressions, iteratables of expressions, or named expressions. To
      pass named expressions use the form name=Expr.

      Example usage: The following will add 4 columns labeled a, b, c, and d::

          df = df.with_columns(
              lit(0).alias('a'),
              [lit(1).alias('b'), lit(2).alias('c')],
              d=lit(3)
              )

      :param exprs: Either a single expression or an iterable of expressions to add.
      :param named_exprs: Named expressions in the form of ``name=expr``

      :returns: DataFrame with the new columns added.



   .. py:method:: write_csv(path: str | pathlib.Path, with_header: bool = False) -> None

      Execute the :py:class:`DataFrame`  and write the results to a CSV file.

      :param path: Path of the CSV file to write.
      :param with_header: If true, output the CSV header row.



   .. py:method:: write_json(path: str | pathlib.Path) -> None

      Execute the :py:class:`DataFrame` and write the results to a JSON file.

      :param path: Path of the JSON file to write.



   .. py:method:: write_parquet(path: str | pathlib.Path, compression: Union[str, Compression] = Compression.ZSTD, compression_level: int | None = None) -> None

      Execute the :py:class:`DataFrame` and write the results to a Parquet file.

      :param path: Path of the Parquet file to write.
      :param compression: Compression type to use. Default is "ZSTD".
                          Available compression types are:
                          - "uncompressed": No compression.
                          - "snappy": Snappy compression.
                          - "gzip": Gzip compression.
                          - "brotli": Brotli compression.
                          - "lz4": LZ4 compression.
                          - "lz4_raw": LZ4_RAW compression.
                          - "zstd": Zstandard compression.
      :param Note: LZO is not yet implemented in arrow-rs and is therefore excluded.
      :param compression_level: Compression level to use. For ZSTD, the
                                recommended range is 1 to 22, with the default being 4. Higher levels
                                provide better compression but slower speed.



   .. py:attribute:: df


